\documentclass[titlepage,11pt,a4paper]{article}

\usepackage{polytechnique}
\usepackage{siunitx}
\usepackage[utf8]{inputenc}
\usepackage[frenchb]{babel}
\usepackage[french]{varioref}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\title{Rapport de MODAL}
\subtitle{Comment faire suivre les murs à un drone}

\author{Youssef \textsc{Achari-Berrada}\\
Basile \textsc{Bruneau}}

\date{Juin 2015}

\begin{document}
\maketitle

%Plan du rapport de MODAL

%Connexion au drone
%Application de l'algorithme lk_tracks au flux vidéo
%Présentation de la stratégie
%Réalisation de plusieurs expériences pour mesurer les vitesses des points (?> courbes, 2D, 3D)
%Régression polynomiale et obtention d'une courbe " théorique "
%Utilisation de cette courbe pour ajuster la trajectoire du drone (algorithme péchu ?> en cours)
%Difficultés, problèmes avec cet algo
%Résultats obtenus

\section*{Introduction}
Nous avions le choix dans ce MODAL entre trois robots : le TurtleBot, Nao et l'AR.Drone. Après avoir vu un aperçu des possibilités offertes par le TurtleBot et l'AR.Drone, notre choix s'est finalement porté sur ce dernier. Même s'il ne disposait pas de caméra de profondeur ou de détecteur de contact par exemple, la possibilité de voler nous semblait plus à même de lui trouver une application utile.

Nous avons en effet décider d'essayer de lui faire surveiller des bâtiments. Concrètement nous souhaitions pouvoir le poser à côté du mur d'un bâtiment, qu'il détecte celui-ci et qu'il suive ensuite les murs pour en faire le tour. Grâce à sa caméra il serait ainsi possible de surveiller les alentours d'une construction, sans installation spécifique (pas de câble à tirer, de capteur à placer...).

\section{Connexion au drone}
Pour faire communiquer l'AR.Drone avec ROS, nous avons utilisé le driver ROS ardrone\_autonomy. Nous l'avons installé depuis les packages Ubuntu, puis il faut ensuite exécuter la commande rosrun ardrone\_autonomy ardrone\_driver pour que les différents topics proposés par le drone soient accessibles.

Deux topics nous intéressaient en particulier :
\begin{itemize}
	\item /ardrone/front/image\_raw : qui permet d'accéder au flux vidéo de la caméra frontale ;
	\item /cmd\_vel : qui permet d'envoyer une instruction de direction à l'AR.Drone (au format geometry\_msgs::Twist).
\end{itemize}

Nous avons également utilisé les topics /ardrone/takeoff, /ardrone/land et /ardrone/reset qui permettent respectivement de faire décoller, atterrir, et demander la réinitialisation du drone après un crash par exemple.

\section{Stratégie}
L'AR.Drone disposant de peu de capteurs (pas de GPS, pas de caméra de profondeur) le seul que nous pouvions utiliser pour réaliser ce projet est la caméra frontale. C'est uniquement à partir de cet unique flux vidéo que nous avons essayé de faire suivre les murs au drone.

L'objectif étant de déplacer le drone, c'est le mouvement ici qui nous intéresse. Nous avons donc décidé d'utiliser le flux optique pour obtenir le mouvement des points de l'image. Nous avons initialement repris l'algorithme lkTracks qui calcule uniquement le mouvement des coins détectés dans l'image (ce qui permet un traitement rapide, contrairement à un calcul du mouvement de tous les points de l'image). Comme nous le verrons plus tard, l'inconvénient est que les situation où aucun coin n'est détecté dans l'image ne sont pas rares.

Ensuite l'observation importante est que lorsque le robot vole et avance en longeant un mur à sa droite (en regardant devant lui) alors les points de l'image les plus à droite ont une vitesse horizontale plus élevée que les points qui sont plus vers le centre de l'image. Et lorsque le drone avance à vitesse constante, le profil des vitesses horizontales doit probablement rester constant : les points à droite auront toujours la même vitesse, et ceux plus vers le centre également. Nous avons donc essayé de trouver ce profil des vitesses lorsque le drone longe un mur à une certaine vitesse, et ensuite en comparant la vitesse des points avec le profil théorique, nous allons essayer de corriger la trajectoire du robot.

\section{Communication avec OpenCV}
L'algorithme lk\_tracks est disponible grâce à OpenCV (calcul du flot optique pour certains points), il nous fallait déjà faire parvenir les images du drone à OpenCV. Nous avons utilisé pour ça CvBridge() qui permet de convertir les données en objet géré par OpenCV2. Ensuite on utilise la méthode imgmsg\_to\_cv2(data, "bgr8") où data correspond aux données directement renvoyées par le topic /ardrone/front/image\_raw.

\begin{lstlisting}
def __init__(self):
	self.bridge = CvBridge()
	self.sub = rospy.Subscriber('/ardrone/front/image_raw', Image, self.callback)
def callback(self,data):
	try:
		frame = self.bridge.imgmsg_to_cv2(data, "bgr8")
	except CvBridgeError, e:
		print e
\end{lstlisting}

\section{Expériences et obtention d'une courbe \og{}pratique\fg{}}
Nous avons donc posé l'AR.Drone sur un TurtleBot que nous avons fait progressé en ligne droite le long d'un mur. Puis nous avons enregistré toutes les vitesses des points détectés par la caméra du drone. Afin d'avoir un maximum d'informations, nous avons ajouté des coins nous même sur le mur. Nous avons répété l'expérience une dizaine de fois, ce qui nous a permis d'obtenir les vitesses d'environ \num{25000} points.

Les résultats sont présentés sur la figure \vref{vitesses-2d}.

\begin{figure}
	\centering
	\caption{\label{vitesses-2d} Vitesse horizontale en fonction de la coordonnée horizontale}
	\includegraphics[scale=0.45]{images/vitesses-2d.png}
\end{figure}

On voit déjà que la vitesse horizontale au centre de l'image est quasiment nulle et qu'elle augmente plus on va vers la droite, ce qui confirme bien notre idée initiale. Nous nous sommes ensuite demandés si la coordonnée verticale avait aussi une importance. Pour ça nous avons donc tracé la vitesse horizontale en fonction de x et y sur la figure \vref{vitesses-3d}.

\begin{figure}
	\centering
	\caption{\label{vitesses-3d} Vitesse horizontale en fonction de x et y}
	\includegraphics[width=0.45\textwidth]{images/vitesses-3d-apercu.png}
	\hfill
	\includegraphics[width=0.45\textwidth]{images/vitesses-3d-selon-y.png}
\end{figure}

Il semble alors que la coordonnée y a une importance, les points en haut de l'image vont moins vite que ceux qui sont en bas.

\section{Régression polynomiale des données}
Afin de pouvoir comparer rapidement la vitesse réelle d'un point avec la vitesse théorique d'un point à la même position si le drone longeait à la bonne vitesse et à la bonne distance un mur, il nous faut obtenir une fonction qui à des coordonnées retourne la vitesse théorique. On ne peut pas consulter les \num{25000} mesures et retourner la vitesse du point le plus proche dont on avait mesuré la vitesse.

Nous avons donc choisi d'effectuer une régression polynomiale pour obtenir l'équation d'une courbe \og{}théorique\fg{}. Nous avons utilisé Scikit qui propose plusieurs outils pour calculer ce genre de chose.

Nous ne lui avons pas donné les \num{25000} points, mais seulement quelques milliers. Nous avons de plus écarté quatre de nos treize expériences, qui nous paraissaient faussées (où le TurtleBot avait beaucoup dévié de sa trajectoire, normalement rectiligne).

\begin{lstlisting}
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
model = Pipeline([('poly', PolynomialFeatures(degree=4)), ('linear',LinearRegression(fit_intercept=False))])
x = [[247.02, 99.29],[286.24, 126.29], ... ]
y = [ ... ]
z = [ ... ]
model = model.fit(x, z)
print model.named_steps['linear'].coef_
# z = 2.28127677e+01 -2.72574113e-01*x -1.49826090e-02*y +7.83386484e-04*x^2 +3.24478877e-04*x*y -2.27923981e-04*y^2
\end{lstlisting}

Finalement voici la courbe que nous avons obtenue, figure \vref{regression}, en demandant un polynôme de degré 2.

\begin{figure}
	\caption{\label{regression} Surface obtenue par régression polynomiale}
	\includegraphics[scale=0.8]{images/surface-regression.png}
\end{figure}


\section{Difficultés et problèmes}
Le principal soucis avec cet algorithme est que très souvent le drone ne mesurait la vitesse d'aucun point. Il lui fallait absolument détecter des angles, et même en accrochant des éléments texturés ou en dessinant des coins sur le mur, à cause de la luminosité et du mauvais positionnement par rapport à la caméra du drone (le mur ne se situe pas en face mais complètement à sa droite) aucun angle n'était détecté. Nous avons donc envisagé plusieurs solutions :
\begin{itemize}
	\item Faire voler le drone en biais, de façon à ce qu'il voit mieux le mur. Cela posait cependant deux problèmes : il voyait moins ce qui se passe devant lui (si on souhaite par exemple surveiller l'environnement cela n'est pas souhaitable) et il nous fallait recommencer nos expériences ;
	\item Changer de stratégie et décider de tracer au sol une ligne qui longerait le mur, et utiliser la deuxième caméra du drone pour qu'il la suive. Mais avec un tel choix le drone serait alors moins autonome, et s'il volait trop haut il ne verrait plus cette ligne ;
	\item Calculer le flot optique pour tous les points de l'image et pas seulement les coins. Cela nécessite beaucoup plus de puissance de calcul, mais on peut espérer de meilleurs résultats. C'est la solution vers laquelle nous nous sommes tournés.
\end{itemize}

Nous avons donc utilisé l'algorithme opt\_flow qui calcule la vitesse de tous les points de l'image. Cependant il est vite apparu qu'il s'agissait d'un calcul beaucoup trop couteux pour l'ordinateur portable Asus que nous avions, qui obtenait les résultats avec un retard de plusieurs secondes. Comme nous nous intéressions uniquement au tiers droit de l'image, nous avons rogné celle-ci, et comme ça n'était pas suffisant, nous avons divisé la résolution par quatre. Cette fois le calcul était effectué en temps réel.

Cela demandait cependant une légère adaptation du code, car nous avions donc une fonction qui à des coordonnées retourne une vitesse théorique, mais après le redimensionnement le l'image, les coordonnées sont faussées. Il faut donc calculer les coordonnées initiales du pixel, avant le redimensionnement de l'image.

\newpage

\section*{Conclusion}
Désormais, nous avons quasiment à tout moment la vitesse instantanée d'un grand nombre de points, que nous pouvons comparer à la courbe théorique. Il reste à améliorer l'algorithme (avec une rétroaction notamment) afin d'obtenir le résultat souhaité.

Le travail n'a pas été facilité par des difficultés techniques (connexion au drone instable, sensibilité au vent en extérieur, difficulté de trouver un drone correctement fonctionnel, batterie assez faible) liées au drone, mais a été une bonne première expérience.

Nous avons maintenant une vision du mode de fonctionnement de la robotique sous l'œil informatique aujourd'hui, et des difficultés qui y sont liées. Et nous avons également eu un bon premier contact avec OpenCV, dont nous n'avons vu qu'une partie mais que nous ne connaissions pas et qui est une bibliothèque très puissante.

\end{document}